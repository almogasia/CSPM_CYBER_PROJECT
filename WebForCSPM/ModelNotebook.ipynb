{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy.stats import norm\n",
    "\n",
    "class DFToDictTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer to convert DataFrame columns to dictionary format for feature hashing\"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.columns].to_dict(orient='records')\n",
    "\n",
    "def build_preprocessor():\n",
    "    \"\"\"Build preprocessing pipeline for numeric and categorical features\"\"\"\n",
    "    numeric_features = ['ip_first_octet', 'ip_second_octet', 'isRootUser', 'has_suspicious_arn']\n",
    "    categorical_features = ['eventName', 'eventSource', 'errorCode']\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('to_dict', DFToDictTransformer(categorical_features)),\n",
    "        ('hasher', FeatureHasher(n_features=256, input_type='dict'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        sparse_threshold=0\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess cloud security log data for model training\"\"\"\n",
    "    df['sourceIPAddress'] = df['sourceIPAddress'].fillna('0.0.0.0')\n",
    "    df['ip_first_octet'] = df['sourceIPAddress'].apply(lambda x: int(str(x).split('.')[0]) if str(x).split('.')[0].isdigit() else 0)\n",
    "    df['ip_second_octet'] = df['sourceIPAddress'].apply(lambda x: int(str(x).split('.')[1]) if len(str(x).split('.')) > 1 and str(x).split('.')[1].isdigit() else 0)\n",
    "\n",
    "    df['userIdentityarn'] = df['userIdentityarn'].fillna('')\n",
    "    df['has_suspicious_arn'] = df['userIdentityarn'].apply(lambda x: 1 if re.search(r'admin|root|super', x, re.IGNORECASE) else 0)\n",
    "    df['isRootUser'] = (df['userIdentitytype'] == 'Root').astype(int)\n",
    "    df['errorCode'] = df['errorCode'].fillna('NoError')\n",
    "\n",
    "    df = df.drop(columns=['eventID', 'userIdentitytype', 'userIdentityarn', 'sourceIPAddress'])\n",
    "    return df\n",
    "\n",
    "def train_autoencoder(X, hidden_layer_sizes=(30, 15, 30), max_iter=100):\n",
    "    \"\"\"Train autoencoder for anomaly detection\"\"\"\n",
    "    if hasattr(X, 'toarray'):\n",
    "        X = X.toarray()\n",
    "    autoencoder = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=max_iter, random_state=42)\n",
    "    autoencoder.fit(X, X)\n",
    "    return autoencoder\n",
    "\n",
    "def train_models(df):\n",
    "    \"\"\"Train multi-model ensemble for cloud security anomaly detection\"\"\"\n",
    "    print(\"Preprocessing data...\")\n",
    "    df = preprocess_data(df)\n",
    "    preprocessor = build_preprocessor()\n",
    "    preprocessor.fit(df)\n",
    "    \n",
    "    X = preprocessor.transform(df)\n",
    "    if hasattr(X, \"toarray\"):\n",
    "        X = X.toarray()\n",
    "\n",
    "    print(\"Training Isolation Forest with optimized contamination...\")\n",
    "    # Use low contamination for anomaly detection\n",
    "    isolation_forest = IsolationForest(\n",
    "        n_estimators=100, \n",
    "        contamination=0.001,  # Only 0.1% should be anomalous\n",
    "        random_state=42,\n",
    "        max_samples='auto'\n",
    "    )\n",
    "    isolation_forest.fit(X)\n",
    "\n",
    "    # Get anomaly scores and find proper threshold\n",
    "    iso_scores = isolation_forest.decision_function(X)\n",
    "    iso_scores_flipped = -iso_scores\n",
    "    \n",
    "    # Use percentiles for different risk levels\n",
    "    iso_threshold_99_9 = np.percentile(iso_scores_flipped, 99.9)\n",
    "    iso_threshold_99 = np.percentile(iso_scores_flipped, 99.0)\n",
    "    iso_threshold_95 = np.percentile(iso_scores_flipped, 95.0)\n",
    "    \n",
    "    print(f\"99.9th percentile threshold: {iso_threshold_99_9:.4f}\")\n",
    "    print(f\"99th percentile threshold: {iso_threshold_99:.4f}\")\n",
    "    print(f\"95th percentile threshold: {iso_threshold_95:.4f}\")\n",
    "    \n",
    "    # Save thresholds for different risk levels\n",
    "    joblib.dump({\n",
    "        'threshold_99_9': iso_threshold_99_9,\n",
    "        'threshold_99': iso_threshold_99,\n",
    "        'threshold_95': iso_threshold_95,\n",
    "        'min_score': iso_scores_flipped.min(),\n",
    "        'max_score': iso_scores_flipped.max()\n",
    "    }, '/kaggle/working/iso_thresholds.joblib')\n",
    "\n",
    "    print(\"Training Random Forest with balanced labels...\")\n",
    "    # Create more balanced labels - only top 0.1% are anomalies\n",
    "    iso_preds = isolation_forest.predict(X)\n",
    "    y = (iso_preds == -1).astype(int)\n",
    "    \n",
    "    print(f\"Anomaly ratio in training: {y.mean():.4f}\")\n",
    "    \n",
    "    # Use class weights to handle extreme imbalance\n",
    "    n_anomalies = y.sum()\n",
    "    n_normal = len(y) - n_anomalies\n",
    "    class_weights = {0: 1, 1: n_normal / n_anomalies}\n",
    "    \n",
    "    random_forest = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42,\n",
    "        class_weight=class_weights,\n",
    "        max_depth=8  # Prevent overfitting\n",
    "    )\n",
    "    random_forest.fit(X, y)\n",
    "\n",
    "    print(\"Training Autoencoder...\")\n",
    "    autoencoder = train_autoencoder(X, hidden_layer_sizes=(30, 15, 30), max_iter=100)\n",
    "\n",
    "    print(\"Calculating Autoencoder thresholds...\")\n",
    "    reconstructions = autoencoder.predict(X)\n",
    "    if reconstructions.ndim == 1:\n",
    "        reconstructions = reconstructions.reshape(-1, 1)\n",
    "    if reconstructions.shape[1] != X.shape[1]:\n",
    "        reconstructions = np.tile(reconstructions, (1, X.shape[1]))\n",
    "\n",
    "    errors = np.mean((X - reconstructions) ** 2, axis=1)\n",
    "    \n",
    "    # Use percentiles for autoencoder too\n",
    "    ae_threshold_99_9 = np.percentile(errors, 99.9)\n",
    "    ae_threshold_99 = np.percentile(errors, 99.0)\n",
    "    ae_threshold_95 = np.percentile(errors, 95.0)\n",
    "    \n",
    "    print(f\"AE 99.9th percentile threshold: {ae_threshold_99_9:.4f}\")\n",
    "    print(f\"AE 99th percentile threshold: {ae_threshold_99:.4f}\")\n",
    "    print(f\"AE 95th percentile threshold: {ae_threshold_95:.4f}\")\n",
    "    \n",
    "    joblib.dump({\n",
    "        'threshold_99_9': ae_threshold_99_9,\n",
    "        'threshold_99': ae_threshold_99,\n",
    "        'threshold_95': ae_threshold_95,\n",
    "        'mean_error': np.mean(errors),\n",
    "        'std_error': np.std(errors)\n",
    "    }, '/kaggle/working/ae_thresholds.joblib')\n",
    "\n",
    "    # Save models\n",
    "    joblib.dump(preprocessor, '/kaggle/working/preprocessor.joblib')\n",
    "    joblib.dump(isolation_forest, '/kaggle/working/isolation_forest.joblib')\n",
    "    joblib.dump(random_forest, '/kaggle/working/random_forest.joblib')\n",
    "    joblib.dump(autoencoder, '/kaggle/working/autoencoder.joblib')\n",
    "\n",
    "    print(\"Models and thresholds saved.\")\n",
    "\n",
    "def score_single_log(log_dict):\n",
    "    \"\"\"Score individual log entries using trained ensemble models\"\"\"\n",
    "    preprocessor = joblib.load('/kaggle/working/preprocessor.joblib')\n",
    "    isolation_forest = joblib.load('/kaggle/working/isolation_forest.joblib')\n",
    "    random_forest = joblib.load('/kaggle/working/random_forest.joblib')\n",
    "    autoencoder = joblib.load('/kaggle/working/autoencoder.joblib')\n",
    "    iso_thresholds = joblib.load('/kaggle/working/iso_thresholds.joblib')\n",
    "    ae_thresholds = joblib.load('/kaggle/working/ae_thresholds.joblib')\n",
    "\n",
    "    df_log = pd.DataFrame([log_dict])\n",
    "    \n",
    "    # Preprocessing\n",
    "    df_log['eventVersion'] = pd.to_numeric(df_log['eventVersion'], errors='coerce').fillna(0)\n",
    "    df_log['userIdentityaccountId'] = pd.to_numeric(df_log['userIdentityaccountId'], errors='coerce').fillna(0)\n",
    "    df_log['sourceIPAddress'] = df_log['sourceIPAddress'].astype(str)\n",
    "    df_log = preprocess_data(df_log)\n",
    "    \n",
    "    X_log = preprocessor.transform(df_log)\n",
    "    if hasattr(X_log, 'toarray'):\n",
    "        X_log = X_log.toarray()\n",
    "\n",
    "    # 1. Isolation Forest - Use percentile-based scoring\n",
    "    iso_pred_raw = isolation_forest.decision_function(X_log)[0]\n",
    "    iso_score = -iso_pred_raw\n",
    "    \n",
    "    # Convert to risk score based on thresholds\n",
    "    if iso_score >= iso_thresholds['threshold_99_9']:\n",
    "        iso_risk = 100  # Critical\n",
    "    elif iso_score >= iso_thresholds['threshold_99']:\n",
    "        iso_risk = 80   # High\n",
    "    elif iso_score >= iso_thresholds['threshold_95']:\n",
    "        iso_risk = 40   # Medium\n",
    "    else:\n",
    "        # Linear interpolation for low scores\n",
    "        normalized = (iso_score - iso_thresholds['min_score']) / (iso_thresholds['threshold_95'] - iso_thresholds['min_score'])\n",
    "        iso_risk = max(0, normalized * 30)  # 0-30 range for normal\n",
    "    \n",
    "    # 2. Random Forest - Use calibrated probabilities\n",
    "    rf_pred_proba = random_forest.predict_proba(X_log)[0][1]\n",
    "    # Convert to risk score (0-100)\n",
    "    rf_risk = rf_pred_proba * 100\n",
    "    \n",
    "    # 3. Autoencoder - Use threshold-based scoring\n",
    "    recon = autoencoder.predict(X_log)\n",
    "    if recon.ndim == 1:\n",
    "        recon = recon.reshape(1, -1)\n",
    "    if recon.shape[1] != X_log.shape[1]:\n",
    "        recon = np.tile(recon, (1, X_log.shape[1]))\n",
    "    \n",
    "    ae_error = np.mean((X_log - recon) ** 2)\n",
    "    \n",
    "    # Convert to risk score based on thresholds\n",
    "    if ae_error >= ae_thresholds['threshold_99_9']:\n",
    "        ae_risk = 100  # Critical\n",
    "    elif ae_error >= ae_thresholds['threshold_99']:\n",
    "        ae_risk = 80   # High\n",
    "    elif ae_error >= ae_thresholds['threshold_95']:\n",
    "        ae_risk = 40   # Medium\n",
    "    else:\n",
    "        # Linear interpolation for low scores\n",
    "        normalized = (ae_error - ae_thresholds['mean_error']) / (ae_thresholds['threshold_95'] - ae_thresholds['mean_error'])\n",
    "        ae_risk = max(0, normalized * 30)  # 0-30 range for normal\n",
    "    \n",
    "    # 4. Combine scores with ensemble weighting\n",
    "    # Give more weight to isolation forest (most reliable for anomalies)\n",
    "    final_score = 0.5 * iso_risk + 0.3 * rf_risk + 0.2 * ae_risk\n",
    "    \n",
    "    # 5. Apply risk transformation\n",
    "    # Most logs should be low risk\n",
    "    if final_score < 20:\n",
    "        final_score = final_score * 0.5  # Reduce low scores further\n",
    "    elif final_score > 80:\n",
    "        final_score = final_score * 1.2  # Increase high scores slightly\n",
    "    \n",
    "    # 6. Round to nearest integer\n",
    "    rounded_score = round(final_score)\n",
    "    \n",
    "    return rounded_score\n",
    "\n",
    "def test_distribution(df, sample_size=1000):\n",
    "    \"\"\"Test the score distribution on a sample of logs\"\"\"\n",
    "    print(f\"Testing distribution on {sample_size} random logs...\")\n",
    "    \n",
    "    test_logs = df.sample(n=sample_size, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for idx, log in test_logs.iterrows():\n",
    "        try:\n",
    "            score = score_single_log(log.to_dict())\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scoring log {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print distribution\n",
    "    print(\"\\n=== SCORE DISTRIBUTION ===\")\n",
    "    print(f\"0-20: {sum(1 for s in scores if s <= 20)} ({sum(1 for s in scores if s <= 20)/len(scores)*100:.1f}%)\")\n",
    "    print(f\"21-40: {sum(1 for s in scores if 21 <= s <= 40)} ({sum(1 for s in scores if 21 <= s <= 40)/len(scores)*100:.1f}%)\")\n",
    "    print(f\"41-60: {sum(1 for s in scores if 41 <= s <= 60)} ({sum(1 for s in scores if 41 <= s <= 60)/len(scores)*100:.1f}%)\")\n",
    "    print(f\"61-80: {sum(1 for s in scores if 61 <= s <= 80)} ({sum(1 for s in scores if 61 <= s <= 80)/len(scores)*100:.1f}%)\")\n",
    "    print(f\"81-100: {sum(1 for s in scores if s >= 81)} ({sum(1 for s in scores if s >= 81)/len(scores)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nMean score: {np.mean(scores):.2f}\")\n",
    "    print(f\"Median score: {np.median(scores):.2f}\")\n",
    "    print(f\"Std score: {np.std(scores):.2f}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== STARTING MODEL TRAINING ===\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df_full = pd.read_csv('/kaggle/input/aws-cloudtrails-dataset-from-flaws-cloud/dec12_18features.csv')\n",
    "    df = df_full.sample(n=100000, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df)} samples\")\n",
    "    \n",
    "    # Train models with optimized parameters\n",
    "    print(\"\\n=== TRAINING MODELS ===\")\n",
    "    train_models(df.copy())\n",
    "    \n",
    "    # Test distribution\n",
    "    print(\"\\n=== TESTING DISTRIBUTION ===\")\n",
    "    scores = test_distribution(df, sample_size=1000)\n",
    "    \n",
    "    # Test a few specific examples\n",
    "    print(\"\\n=== TESTING SPECIFIC EXAMPLES ===\")\n",
    "    example_logs = df.sample(n=5, random_state=123)\n",
    "    \n",
    "    for idx, log in example_logs.iterrows():\n",
    "        try:\n",
    "            score = score_single_log(log.to_dict())\n",
    "            event_name = log.get('eventName', 'Unknown')\n",
    "            user_type = log.get('userIdentitytype', 'Unknown')\n",
    "            print(f\"Log {idx}: {event_name} by {user_type} -> Score: {score}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with log {idx}: {e}\")\n",
    "    \n",
    "    print(\"\\n=== TRAINING COMPLETE ===\")\n",
    "    print(\"Models saved to /kaggle/working/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_log_string(log_string):\n",
    "    \"\"\"\n",
    "    Convert a pipe-separated log string to dictionary format for scoring\n",
    "    \n",
    "    Expected format: feature1|feature2|feature3|...|feature18\n",
    "    \"\"\"\n",
    "    columns = [\n",
    "        \"eventID\", \"eventTime\", \"sourceIPAddress\", \"userAgent\", \"eventName\",\n",
    "        \"eventSource\", \"awsRegion\", \"eventVersion\", \"userIdentitytype\",\n",
    "        \"eventType\", \"userIdentityaccountId\", \"userIdentityprincipalId\",\n",
    "        \"userIdentityarn\", \"userIdentityaccessKeyId\", \"userIdentityuserName\",\n",
    "        \"errorCode\", \"errorMessage\", \"requestParametersinstanceType\"\n",
    "    ]\n",
    "    \n",
    "    # Split the string by pipe\n",
    "    values = log_string.strip().split('|')\n",
    "    \n",
    "    # Check if we have the right number of features\n",
    "    if len(values) != len(columns):\n",
    "        raise ValueError(f\"Expected {len(columns)} features, got {len(values)}\")\n",
    "    \n",
    "    # Create dictionary\n",
    "    log_dict = dict(zip(columns, values))\n",
    "    \n",
    "    return log_dict\n",
    "\n",
    "def score_log_string(log_string):\n",
    "    \"\"\"\n",
    "    Score a pipe-separated log string directly\n",
    "    \"\"\"\n",
    "    log_dict = parse_log_string(log_string)\n",
    "    return score_single_log_corrected(log_dict)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    log_string = \"9e6fb24d-e84d-40dd-8975-83a4f5c7f902|2018-07|253.6|aws-cli|DescribeSnapshots|ec2.amazonaws.com|us-west-2|1.05|IAMUser|AwsApiCall|811596193553.0|AIDA9BO36HFBHKGJAO9C1|arn:aws:iam::811596193553:user/backup|AKIA01U43UX3RBRDXF4Q|backup|NaN|NoError|NotApplicable\"\n",
    "\n",
    "    score = score_log_string(log_string)\n",
    "    print(f\"Risk score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
